{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training a self-supervised model: Contrastive Predictive Coding (CPC)\n",
    "\n",
    "In this notebook, we will train a self-supervised model using the Contrastive Predictive Coding (CPC) method. \n",
    "This method is based on the idea of predicting future tokens in a sequence, and it has been shown to be very effective in learning useful representations for downstream tasks.\n",
    "This framework already provides an implementation of CPC, so we will use it to train the model.\n",
    "\n",
    "We will pre-train the model using KuHar dataset, and then we will use the learned representations to train a classifier for the downstream task (fine tuning). \n",
    "For both stages of training, as the last notebook, we will:\n",
    "\n",
    "1. Create a `Dataset` and then `LightningDataModule` to load the data;\n",
    "2. Instantiate the CPC model; and\n",
    "3. Train the model using PyTorch Lightning.\n",
    "\n",
    "Every SSL model in this framework can instantiate in two ways:\n",
    "\n",
    "1. Instantiate each element, such as the encoder, the autoregressive model, and the CPC model, and then pass them to the CPC model; or\n",
    "2. Using builder methods to instantiate the model. In this case, we do not need to instantiate each element separately, but we can still customize the model by passing the desired parameters to the builder methods. This is the approach we will use in this notebook.\n",
    "\n",
    "In summary, the second approach encapsulates the first one, making it easier to use and it is more convenient for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training the model\n",
    "\n",
    "We will pre-train the model using the KuHar dataset. CPC is a self-supervised method, so we do not need labels to train the model. \n",
    "However, CPC assumes that the input data is sequential, that is, an input is a sequence of time-steps comprising different acitivities. \n",
    "Thus, for HAR, usually, one sample is a multi-modal time-series correspond to the whole time-series of a single user.\n",
    "\n",
    "### Creating the LightningDataModule\n",
    "\n",
    "Our dataset must be organized in the following way:\n",
    "\n",
    "```\n",
    "data/\n",
    "    train/\n",
    "        user1.csv\n",
    "        user2.csv\n",
    "        ...\n",
    "    validation/\n",
    "        user4.csv\n",
    "        user5.csv\n",
    "        ...\n",
    "    test/\n",
    "        user6.csv\n",
    "        user7.csv\n",
    "        ...\n",
    "```\n",
    "\n",
    "And the content of each CSV file should be something like:\n",
    "\n",
    "| timestamp | accel-x | accel-y | accel-z | gyro-x | gyro-y | gyro-z | activity  |\n",
    "|-----------|---------|---------|---------|--------|--------|--------|-----------|\n",
    "| 0         | 0.1     | 0.2     | 0.3     | 0.4    | 0.5    | 0.6    | 0         |\n",
    "| 1         | 0.2     | 0.3     | 0.4     | 0.5    | 0.6    | 0.7    | 0         |\n",
    "| ...       | ...     | ...     | ...     | ...    | ...    | ...    | ...       |\n",
    "\n",
    "Where `timestamp` is the time-stamp of the sample, `accel-x`, `accel-y`, `accel-z`, `gyro-x`, `gyro-y`, and `gyro-z` are the features of the sample, and `activity` is the label of the time-step.\n",
    "\n",
    "In this way, we should use the `SeriesFolderCSVDataset` to load the data.\n",
    "This will create a `Dataset` for us, where each CSV file is a sample, and each row of the CSV file is a time-step, and the columns are the features.\n",
    "\n",
    "If your data is organized as above, where inside the root folder (`data/` in this case) there are sub-folders for each split (`train/`, `validation/`, and `test/`), and inside each split folder there are the CSV files, you can use the `UserActivityFolderDataModule` to create a `LightningDataModule` for you.\n",
    "This class will create `DataLoader` of `SeriesFolderCSVDataset` for each split (train, validation, and test), and will setup data correctly.\n",
    "\n",
    "In this notebook, we will use the `UserActivityFolderDataModule` to create the `LightningDataModule` for us. This class requires the following parameters:\n",
    "\n",
    "- `data_path`: the root directory of the data;\n",
    "- `features`: the name of the features columns;\n",
    "- `pad`: a boolean indicating if the samples should be padded to the same length, that is, the length of the longest sample in the dataset. The padding scheme will replicate the samples, from the beginning, until the length of the longest sample is reached. \n",
    "  \n",
    "> **NOTE**: The samples may have different lengths, so, for this method, the `batch_size` must be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1706884475.353781] [aae107fc745c:2265333:f]        vfs_fuse.c:281  UCX  ERROR inotify_add_watch(/tmp) failed: No space left on device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UserActivityFolderDataModule(data_path=/workspaces/hiaac-m4/data/view_concatenated/KuHar_cpc, batch_size=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from ssl_tools.data.data_modules import UserActivityFolderDataModule\n",
    "\n",
    "data_path = \"/workspaces/hiaac-m4/data/view_concatenated/KuHar_cpc\"\n",
    "\n",
    "data_module = UserActivityFolderDataModule(\n",
    "    data_path, \n",
    "    features=(\"accel-x\", \"accel-y\", \"accel-z\", \"gyro-x\", \"gyro-y\", \"gyro-z\"),\n",
    "    batch_size=1,       # We set to 1 for CPC\n",
    "    label=None,         # We do not want to return the labels, only data.\n",
    "    pad=False           # If you want padded data, set it to True. \n",
    "                        #   This guarantees that all data have the same length.     \n",
    ")\n",
    "\n",
    "data_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-training the model\n",
    "\n",
    "Here we will use the builder method `build_cpc` to instantiate the CPC model.\n",
    "This will instantiate an CPC self-supervised model, with the default encoder (`ssl_tools.models.layers.gru.GRUEncoder`), that is an GRU+Linear, and the default autoregressive model (`torch.nn.GRU`).\n",
    "\n",
    "We can parametrize the creation of the model by passing the desired parameters to the builder method. T\n",
    "he `build_cpc` method can be parametrized the following parameters:\n",
    "\n",
    "- `encoding_size`: the size of the encoded representation;\n",
    "- `in_channels`: number of input features;\n",
    "- `gru_hidden_size`: number of features in the hidden state of the GRU;\n",
    "- `gru_num_layers`: number of layers in the GRU;\n",
    "- `learning_rate`: the learning rate of the optimizer;\n",
    "- `window_size` : size of the input windows (`X_t`) to be fed to the encoder (GRU).\n",
    "\n",
    "All parameters are optional, and have default values. \n",
    "You may want to consult the documentation of the method to see the default values and additional parameters.\n",
    "\n",
    "Note that the `LightningModule` returned by the `build_cpc` method is already configured to use the CPC loss, and the `Adam` optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CPC(\n",
       "  (encoder): GRUEncoder(\n",
       "    (rnn): GRU(6, 100, bidirectional=True)\n",
       "    (nn): Linear(in_features=200, out_features=128, bias=True)\n",
       "  )\n",
       "  (density_estimator): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (auto_regressor): GRU(128, 128, batch_first=True)\n",
       "  (loss_func): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ssl_tools.models.ssl.cpc import build_cpc\n",
    "encoding_size = 128     \n",
    "in_channels = 6\n",
    "gru_hidden_size = 100\n",
    "gru_num_layers = 1\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = build_cpc(\n",
    "    encoding_size=encoding_size,\n",
    "    in_channels=in_channels,\n",
    "    gru_hidden_size=gru_hidden_size,\n",
    "    gru_num_layers=gru_num_layers,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the Trainer and call the `fit` method to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name              | Type             | Params\n",
      "-------------------------------------------------------\n",
      "0 | encoder           | GRUEncoder       | 90.5 K\n",
      "1 | density_estimator | Linear           | 16.5 K\n",
      "2 | auto_regressor    | GRU              | 99.1 K\n",
      "3 | loss_func         | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------------\n",
      "206 K     Trainable params\n",
      "0         Non-trainable params\n",
      "206 K     Total params\n",
      "0.824     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3634f46d0440d78d1cc4df789b6f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640b0f0a79794c2d97ca00a311e4b08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36aa3a12d53f47e39962e445c39d2af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b41ae7b312341bb8872f7b4e56a753e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcc4c7c3da648fa94d7a3f2663ffe5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f89209f2f5b44488be9b5d002fb949f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52322530fcc641b2b3f2da9e76d23a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268860b5d4104d1f83692fe48f173f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c45fd34de246cea3b0321434c8205a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66aa47a3e2e34cd383f03c62dde9bbaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ce5719fcd540dfad7536ac5ac75baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e472b12bf71c4b2f909f481385a2b371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "\n",
    "max_epochs = 10\n",
    "trainer = L.Trainer(max_epochs=max_epochs)\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This finishes the pre-training stage. \n",
    "\n",
    "To obtain the latent representations of the data, we must use `model.forward()` method on the data. \n",
    "In this framework, the `forward` method of the SSL models returns the latent representations of the input data. \n",
    "Usually this is the output of the encoder, as in this case, but it may vary depending on the model.\n",
    "\n",
    "We will use the encoder to obtain the latent representations of the data, and then we will use these representations to train a classifier for the downstream task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model\n",
    "\n",
    "After pre-training the model, we will use the learned representations to train a classifier for the downstream task, in this case, the HAR task. \n",
    "\n",
    "> **NOTE**: It is important that the SSL models implement the `forward` method to return the latent representations of the input data, so we can use these representations to train the classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the LightningDataModule\n",
    "\n",
    "Human acivity recognition is a supervised classification task, that usually receives multi-modal windowed time-series as input, diferently from the self-supervised task, that receives the whole time-series of a single user.\n",
    "Thus, we cannot use the same `LightningDataModule` to load the data for the downstream task. \n",
    "\n",
    "In this notebook, we will use the windowed time-series version of the KuHar dataset, that each split is a single CSV file, containing windowed time-series of the users. \n",
    "The content of the file should be something like:\n",
    "\n",
    "```\n",
    "KuHar/\n",
    "    train.csv\n",
    "    validation.csv\n",
    "    test.csv\n",
    "```\n",
    "\n",
    "The CSVs file may look like this:\n",
    "\n",
    "| accel-x-0 | accel-x-1 | accel-y-0 | accel-y-1 |  class |\n",
    "|-----------|-----------|-----------|-----------|--------|\n",
    "| 0.502123  | 0.02123   | 0.502123  | 0.502123  |  0     |\n",
    "| 0.6820123 | 0.02123   | 0.502123  | 0.502123  |  1     |\n",
    "| 0.498217  | 0.00001   | 1.414141  | 3.141592  |  1     |\n",
    "\n",
    "As each CSV file contains time-windows signals of two 3-axis sensors (accelerometer and gyroscope), we must use the `MultiModalSeriesCSVDataset` class. \n",
    "\n",
    "As in last notebook, we will use the `MultiModalHARSeriesDataModule` to facilitate the creation of the `LightningDataModule`. This class will create `DataLoader` of `MultiModalSeriesCSVDataset` for each split (train, validation, and test), and will setup data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiModalHARSeriesDataModule(data_path=/workspaces/hiaac-m4/ssl_tools/data/standartized_balanced/KuHar, batch_size=64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ssl_tools.data.data_modules.har import MultiModalHARSeriesDataModule\n",
    "\n",
    "data_path = \"/workspaces/hiaac-m4/ssl_tools/data/standartized_balanced/KuHar/\"\n",
    "\n",
    "data_module = MultiModalHARSeriesDataModule(\n",
    "    data_path=data_path,\n",
    "    feature_prefixes=(\"accel-x\", \"accel-y\", \"accel-z\", \"gyro-x\", \"gyro-y\", \"gyro-z\"),\n",
    "    label=\"standard activity code\",\n",
    "    features_as_channels=True,\n",
    "    batch_size=64,\n",
    "    num_workers=0,  # Sequential, for notebook compatibility\n",
    ")\n",
    "data_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning the model\n",
    "\n",
    "A model for a downstream task is usually composed of two parts: the backbone model, that is the model that generates the representations of the input data, *i.e.*, the encoder, and the prediction head, which is the model that receives the representations and outputs the predictions, usually, a MLP.\n",
    "\n",
    "To handle the fine-tune process, we can design a new model, that is composed of the pre-trained backbone and the prediction head, and then train this new model with the labeled data. \n",
    "In order to facilitate this process, this framework provides the `SSLDiscriminator` class, that receives the backbone model and the prediction head, and then trains the classifier with the labeled data.\n",
    "\n",
    "In summary, the `SSLDiscriminator` class is a `LightningModule` that generate the representations of the input data using the backbone model, that is, using the `forward` method of the pre-trained backbone model, and then uses the prediction head to output the predictions, something like `y_hat = prediction_head(backbone(sample))`. \n",
    "The predictions and labels are then used to compute the loss and train the model. \n",
    "By default, the `SSLDiscriminator` is trained using the `Adam` optimizer with parametrizable `learning_rate`.\n",
    "\n",
    "It worth to mention that the `SSLDiscriminator` class `forward` method receives the input data and the labels, and returns the predictions. \n",
    "This is different from the `forward` method of the self-supervised models, that receives only the input data and returns the latent representations of the input data.\n",
    "\n",
    "It worth to notice that the fine-tune train process can be done in two ways: \n",
    "\n",
    "1. Fine-tuning the whole model, that is, backbone (encoder) and classifier, with the labeled data; or \n",
    "2. Fine-tuning only the classifier, with the labeled data.\n",
    "\n",
    "The `SSLDisriminator` class can handle both cases, with the `update_backbone` parameter. \n",
    "If `update_backbone` is `True`, the whole model is fine-tuned (case 1, above), otherwise, only the classifier is fine-tuned (case 2, above).\n",
    "\n",
    "Let's create our prediction head and `SSLDisriminator` model and train it with the labeled data. \n",
    "Prediction heads for most popular tasks are already implemented in the `ssl_tools.models.ssl.modules.heads` module. \n",
    "In this notebook, we will use the `CPCPredictionHead` prediction head, that is a MLP with 3 hidden layers and dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CPCPredictionHead(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (4): Linear(in_features=64, out_features=6, bias=True)\n",
       "    (5): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ssl_tools.models.ssl.classifier import SSLDiscriminator\n",
    "from ssl_tools.models.ssl.modules.heads import CPCPredictionHead\n",
    "\n",
    "number_of_classes = 6\n",
    "\n",
    "prediction_head = CPCPredictionHead(\n",
    "    input_dim=encoding_size,                # Size of the encoding (input)\n",
    "    hidden_dim1=64,\n",
    "    hidden_dim2=64,\n",
    "    output_dim=number_of_classes            # Number of classes\n",
    ")\n",
    "\n",
    "prediction_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the `SSLDisriminator` model. This class requires the following parameters:\n",
    "\n",
    "- `backbone`: the backbone model, that is, the pre-trained model;\n",
    "- `head`: the prediction head model;\n",
    "- `loss_fn`: the loss function to be used to train the model;\n",
    "\n",
    "Also, we can attach metrics that will be calculated with for every batch of `validation` and `test` sets. \n",
    "The metrics is passed using the `metrics` parameter of the `SSLDisriminator` class, that receives a dictionary with the name of the metric as key and the `torchmetrics.Metric` as value.\n",
    "\n",
    "Let's create the `SSLDiscriminator` and attach the `Accuracy` metric to the model, to check the validation accuracy per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SSLDiscriminator(\n",
       "  (backbone): CPC(\n",
       "    (encoder): GRUEncoder(\n",
       "      (rnn): GRU(6, 100, bidirectional=True)\n",
       "      (nn): Linear(in_features=200, out_features=128, bias=True)\n",
       "    )\n",
       "    (density_estimator): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (auto_regressor): GRU(128, 128, batch_first=True)\n",
       "    (loss_func): CrossEntropyLoss()\n",
       "  )\n",
       "  (head): CPCPredictionHead(\n",
       "    (layers): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Sequential(\n",
       "        (0): ReLU()\n",
       "        (1): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (4): Linear(in_features=64, out_features=6, bias=True)\n",
       "      (5): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchmetrics import Accuracy\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "acc_metric = Accuracy(\n",
    "    task=\"multiclass\",              # We are working with a multiclass\n",
    "                                    #   classification, not a binary one.\n",
    "    num_classes=number_of_classes   # Number of classes\n",
    ")\n",
    "\n",
    "ssl_discriminator = SSLDiscriminator(\n",
    "    backbone=model,                 # The model we trained before (CPC)\n",
    "    head=prediction_head,           # The prediction head we just created\n",
    "    loss_fn=CrossEntropyLoss(),     # The loss function\n",
    "    learning_rate=1e-3,\n",
    "    update_backbone=False,          # We do not want to update the backbone\n",
    "    metrics={\"acc\": acc_metric}     # We want to track the accuracy\n",
    ")\n",
    "ssl_discriminator    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can instantiate the Trainer and call the `fit` method to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name     | Type              | Params\n",
      "-----------------------------------------------\n",
      "0 | backbone | CPC               | 206 K \n",
      "1 | head     | CPCPredictionHead | 12.8 K\n",
      "2 | loss_fn  | CrossEntropyLoss  | 0     \n",
      "-----------------------------------------------\n",
      "12.8 K    Trainable params\n",
      "206 K     Non-trainable params\n",
      "218 K     Total params\n",
      "0.876     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a9bc62bb02433c814bb266ad167222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (22) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a315d4c45dc43baac89162331a17467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a43e85394ce34b1f987664c15e50fd30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae46f2bf6c324fe7b442a2f88f27a28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242bcdc871da408eaf38a5a56334a8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1970a89b26fd4cf39c0ca393b9b6c027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bdbd86f03074702a869b386d177fb7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2167f1510bb4d64a54da935335ccc21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3e63d7f34f4dd2b0dc31467dd464dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e4bfc1a324439a8ecb59e82b9e810f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220928c965124f8fa5d5feeb1ec05f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0696c0f6a144909060b0ab7d53b45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "\n",
    "max_epochs = 10\n",
    "trainer = L.Trainer(max_epochs=max_epochs)\n",
    "trainer.fit(ssl_discriminator, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the model using the test set. If we have added the `Accuracy` metric to the model, it will calculate the accuracy of the model on the test set.\n",
    "All logged metrics will be returnet by `.test()` method, as a list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b0536249cb4290abbe1fb6367f6de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5277777910232544     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    1.4903016090393066     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5277777910232544    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.4903016090393066    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1.4903016090393066, 'test_acc': 0.5277777910232544}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = trainer.test(ssl_discriminator, data_module)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if we want to get the predictions of the model, we can:\n",
    "\n",
    "1. Call the `forward` method of the model, passing the input data (iterating over all batches of the dataloader); or \n",
    "2. Use the `Trainer.predict` method, passing the data module. If you use the `Trainer.predict` method, the model will be set to evaluation mode, and the predictions will be done using the `predict_dataloader` defined in the `LightningDataModule`. This is usually the test set (`test_dataloader`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64924b90def24c998374ee4e9ebd8478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([144, 6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = trainer.predict(ssl_discriminator, data_module)\n",
    "# predictions is a list of tensors. Let's concatenate them.\n",
    "y_hat = torch.cat(y_hat)\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "This notebook comprises the whole process of training a self-supervised model and then using the learned representations to train a classifier for the downstream task.\n",
    "\n",
    "We can standardize this process to facilitate the reproduction of the experiments, and then use it to train different models and evaluate them on different datasets.\n",
    "\n",
    "Nextly we will explore the `Experiment` API that is designed to simplify the process of training and evaluating models, besides of provide a standard way to log the results, save and load models, and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
